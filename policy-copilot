# ğŸ§© Install required libraries
!pip install pdfminer.six langchain sentence-transformers faiss-cpu openai tqdm

# ğŸ§  Imports
import os
import json
from tqdm import tqdm
from typing import List
from pdfminer.high_level import extract_text
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

# ğŸ“ PDF Directory
PDF_DIR = "./policy_docs"
os.makedirs(PDF_DIR, exist_ok=True)

# ğŸ“„ Extract raw text (line by line) using pdfminer
def extract_lines_pdfminer(pdf_path: str) -> List[str]:
    text = extract_text(pdf_path)
    lines = text.split("\n")
    return [line.strip() for line in lines if line.strip()]

# ğŸ”  Heuristic subheading splitter (optional)
def chunk_lines(lines: List[str], chunk_size=5):
    chunks = []
    current_heading = "General"
    for i in range(0, len(lines), chunk_size):
        chunk = lines[i:i + chunk_size]
        if len(chunk[0].split()) < 10 and chunk[0].isupper():
            current_heading = chunk[0]
            chunk = chunk[1:]
        text = current_heading + "\n" + " ".join(chunk)
        chunks.append({
            "text": text,
            "heading": current_heading,
            "line_start": i + 1,
            "line_end": i + len(chunk)
        })
    return chunks

# ğŸ§  Build LangChain Document objects
all_docs = []
for fname in tqdm(os.listdir(PDF_DIR)):
    if not fname.lower().endswith(".pdf"):
        continue
    fpath = os.path.join(PDF_DIR, fname)
    lines = extract_lines_pdfminer(fpath)
    chunks = chunk_lines(lines)
    for chunk in chunks:
        metadata = {
            "document": fname,
            "section": chunk["heading"],
            "line_start": chunk["line_start"],
            "line_end": chunk["line_end"]
        }
        all_docs.append(Document(page_content=chunk["text"], metadata=metadata))

# ğŸ” Embed and index
embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(all_docs, embedding_model)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# ğŸ¤– QA chain
os.environ["OPENAI_API_KEY"] = "sk-..."  # Replace with your actual key
qa = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(model="gpt-4", temperature=0),
    retriever=retriever,
    return_source_documents=True
)

# â“ Ask a question
query = "What is the refund timeline for cancellation?"
response = qa(query)

# ğŸ“‹ Print result
print("ğŸ” Answer:\n", response["result"])
print("\nğŸ“š Sources:")
for doc in response["source_documents"]:
    md = doc.metadata
    print(f"- ğŸ“„ {md['document']} | Section: {md['section']} | Lines: {md['line_start']}-{md['line_end']}")




import os
import requests

# Create model directory
os.makedirs("./models", exist_ok=True)

# Model URL (Q4_0 version from Hugging Face)
url = "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_0.gguf"
local_path = "./models/mistral-7b-instruct-v0.1.Q4_0.gguf"

# Download with streaming
with requests.get(url, stream=True) as r:
    r.raise_for_status()
    with open(local_path, "wb") as f:
        for chunk in r.iter_content(chunk_size=8192):
            f.write(chunk)

print("âœ… Download complete:", local_path)
